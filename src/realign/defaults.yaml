
llm_agents:
  default: &basic_agent_settings
    model: openai/gpt-4o-mini
    system_prompt: |
      Be a good agent.

  rating_5_star_agent: &rating_5_star_agent
    model: openai/gpt-4o-mini
    template: rating_5_star
    json_mode: on
    hyperparams:
      temperature: 0

  explanation_summary_agent: &explanation_summary_agent
    model: openai/gpt-4o-mini
    template: explanation_summary
    json_mode: on
    hyperparams:
      temperature: 0

  pairwise_judge_agent: &pairwise_judge_agent
    model: openai/gpt-4o-mini
    template: |
      Based on the given criteria and {{ choices | length }} choices, rank them in order, starting with the choice that fits the criteria best, and ending the choice that fits the criteria worst. 
      
      Finally, respond in JSON format {"best_choice": "2", "worst_choice": "1"} with your best and worst choices. For example, if the choice 3 is the best choice and choice 5 is the worst, respond {"best_choice": "3", "worst_choice": "5"}.

      [Criteria]
      {{ criteria }}

      {% for choice in choices %}
          [Choice {{ loop.index }}]
          {{ choice }}
      {% endfor %}

      [Selection]
    json_mode: on
    hyperparams:
      temperature: 0

evaluators:

  # hf
  hf_pipeline: 
  hf_label_score_aggregator:

  hf_hate_speech:
    wraps: hf_pipeline
    task: text-classification
    model: facebook/roberta-hate-speech-dynabench-r4-target

    checker: value['label'] in target
    target: [nothate]
    asserts: off

  hf_sentiment_classifier:
    wraps: hf_pipeline
    task: text-classification
    model: cardiffnlp/twitter-roberta-base-sentiment-latest

    checker: value['label'] in target
    target: [positive, neutral]
    asserts: off

  # checkers
  numrange:

  # llm
  allm_rating_json:
    repeat: 3
    aggregate: allm_rating_json_aggregate
    agent_settings: *rating_5_star_agent

  allm_pairwise_judge:
    batch_size: 2
    agent_settings: *pairwise_judge_agent

  allm_rating_json_aggregate:

  # rag
  ragas_pipeline:

  ragas_faithfulness:
    wraps: ragas_pipeline
    metrics: faithfulness

  ragas_answer_relevancy:
    wraps: ragas_pipeline
    metrics: answer_relevancy

  ragas_context_precision:
    wraps: ragas_pipeline
    metrics: context_precision

  ragas_context_recall:
    wraps: ragas_pipeline
    metrics: context_recall

  ragas_context_entity_recall:
    wraps: ragas_pipeline
    metrics: context_entity_recall
  
  ragas_noise_sensitivity_relevant:
    wraps: ragas_pipeline
    metrics: noise_sensitivity_relevant

  ragas_noise_sensitivity_irrelevant:
    wraps: ragas_pipeline
    metrics: noise_sensitivity_irrelevant

  ragas_answer_similarity:
    wraps: ragas_pipeline
    metrics: answer_similarity
  
  ragas_answer_correctness:
    wraps: ragas_pipeline
    metrics: answer_correctness

  ragas_reference_free_rubrics_score:
    wraps: ragas_pipeline
    metrics: reference_free_rubrics_score

  ragas_labelled_rubrics_score:
    wraps: ragas_pipeline
    metrics: labelled_rubrics_score

  ragas_summarization_score:
    wraps: ragas_pipeline
    metrics: summarization_score

  # nlp
  word_count:
  character_count:
  token_count:
  chat_word_count_by_role:
  levenshtein_distance:
  cosine_similarity:
  keyword_assertion:

  # stats
  weighted_sum:
  weighted_mean:

  elo_ratings:
    initial_rating: 1500
    k_factor: 32

